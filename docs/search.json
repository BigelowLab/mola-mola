[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Brought to you by the Tandy Center for Ocean Forecasting at Bigelow Laboratory for Ocean Science.\n\n\n\n Back to top"
  },
  {
    "objectID": "modeling-03.html",
    "href": "modeling-03.html",
    "title": "Modeling with splits",
    "section": "",
    "text": "1 Split into training and testing groups\nLet’s split the our presence observations into training (80%) and testing (20%) groups.\nWe build our models models on the training groups, and, later, we can see that the model might predict for the test cases.\nWe will make a vector of “training”/“testing” flags to indicate which are training and which are for testing for our obserabtions. Note we set the seed to a fixed number so that the random selection is reproducible. Finally, we add this grouping to each table and then re-save for future use.\n\nsource(\"setup.R\")\n\nobs = sf::read_sf(file.path(\"data\", \"obs\", \"obs-covariates.gpkg\"))\n\nobs_flag = rep(\"training\", nrow(obs))\nset.seed(1234)\nix_test &lt;- sample(nrow(obs), nrow(obs) * 0.2)\nobs_flag[ix_test] = \"testing\"\nobs = dplyr::mutate(obs, grouping = obs_flag) |&gt;\n  sf::write_sf(file.path(\"data\", \"obs\", \"obs-covariates.gpkg\"))\n\n\n\n2 Model with training data\nNow we can proceed, much as we did with our basic model, but using only the training portion of the data. Note that this will be model version v3.0.\nYou can see below that we only split the observations and not the background. You can try splitting the background, too.\n\nmodel_path = file.path(\"data\", \"model\", \"v3\", \"v3.0\")\nok = dir.create(model_path, recursive = TRUE, showWarnings = FALSE)\n\nobs_train = dplyr::filter(obs, grouping == \"training\") |&gt;\n  sf::st_drop_geometry() |&gt;\n  dplyr::select(\"sst\", \"u_wind\", \"v_wind\") |&gt;\n  na.omit()\nbkg = sf::read_sf(file.path(\"data\", \"bkg\", \"bkg-covariates.gpkg\")) |&gt;\n  sf::st_drop_geometry() |&gt;\n  dplyr::select(\"sst\", \"u_wind\", \"v_wind\") |&gt;\n  na.omit()\n  \ntrain = dplyr::bind_rows(obs_train, bkg)\nflags = c(rep(1, nrow(obs_train)), rep(0, nrow(bkg)))\n\n\nmodel = maxnet::maxnet(flags, train) |&gt;\n  maxnetic::write_maxnet(file.path(model_path, \"model_v3.0.rds\"))\n\nplot(model, type = \"cloglog\")\n\n\n\n\nThese response plots look remarkably like the ones from our basic model, which should not come as a surprise. But let’s now predict with all of the data, and compute AUC with just our 20% testing data set.\n\n\n\n\n\n\nNote\n\n\n\nNote that the input to predict (below) includes an extra variable, grouping, which is not a covariate used to build the model. This implementation of predict() is perfectly happy to have extra variables in the input, as long as the covariates used to build the model are present.\n\n\n\ninput_obs = obs |&gt;\n  sf::st_drop_geometry() |&gt;\n  dplyr::select(\"sst\", \"u_wind\", \"v_wind\", \"grouping\") |&gt;\n  na.omit()\n\ninput = input_obs |&gt;\n  dplyr::bind_rows(dplyr::mutate(bkg, grouping = \"background\"))\n\npred = predict(model, input, type = \"cloglog\") |&gt;\n  as.vector()\n\npred_test = pred[input$grouping == \"testing\"]\n\npauc = maxnetic::pAUC(pred, pred_test)\nplot(pauc, title = \"Testing data pAUC\")\n\n\n\n\nThat’s pretty good - no significant change in the predictive performance.\nWe can also see how this might work using raster inputs to predict. We’ll once again apply the model to the months of 2019. We read in thr databases, filter those and then read the predictors.\n\ndates = as.Date(c(\"2019-01-01\", \"2019-12-31\"))\n\nsst_path = \"data/oisst\"\nsst_db = oisster::read_database(sst_path) |&gt;\n  dplyr::arrange(date) |&gt;\n  dplyr::filter(dplyr::between(date, dates[1], dates[2]))\n\nwind_path = \"data/nbs\"\nwind_db = nbs::read_database(wind_path) |&gt;\n  dplyr::arrange(date)|&gt;\n  dplyr::filter(dplyr::between(date, dates[1], dates[2]))\n\nu_wind_db = wind_db |&gt;\n  dplyr::filter(param == \"u_wind\")|&gt;\n  dplyr::filter(dplyr::between(date, dates[1], dates[2]))\n\nv_wind_db = wind_db |&gt;\n  dplyr::filter(param == \"v_wind\")|&gt;\n  dplyr::filter(dplyr::between(date, dates[1], dates[2]))\n\npredictors = read_predictors(sst_db = sst_db,\n                             u_wind_db = u_wind_db,\n                             v_wind_db = v_wind_db)\n\nNow we make the predictions with the raster covariates.\n\npred = predict(model, predictors, type = 'cloglog')\n\ncoast = rnaturalearth::ne_coastline(scale = 'large', returnclass = 'sf') |&gt;\n  sf::st_crop(pred)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nplot_coast = function() {\n  plot(sf::st_geometry(coast), col = 'green', add = TRUE)\n}\nplot(pred, hook = plot_coast)\n\n\n\n\nWell, no surprises in the appearance. Next, we compute and plot the ROCs.\n\npauc = maxnetic::pAUC(pred, \n                      dplyr::filter(obs, grouping == 'testing'), \n                      time_column = 'month_id')\nplot(pauc, title = \"Testing data with rasters\")\n\n\n\n\nWhoa! It looks different, but has the same AUC. How’s that possible? Well, in each use of maxnetic::pAUC() we are providing the same presence values; it doesn’t look like from a workflow standpoint (point data vs raster data), but we are predicting the same values with the same model. These presence point values provide the same way points over which the area is computed regardless of which workflow we used. It is true the “universe” of predicted values changed as we switched from points to rasters, and that gives rise to the textual change in the curve (in this case smooth vs bumpy). But area captured under the curve remains the same. Surprising!\nLet’s flip the situation by computing AUC for the training data using the raster predictors.\n\npauc = maxnetic::pAUC(pred, \n                      dplyr::filter(obs, grouping == 'training'), \n                      time_column = 'month_id')\nplot(pauc, title = 'Training data with rasters')\n\n\n\n\nIt’s a slightly higher AUC, given that the model was trained on that data and that there are more points it isn’t too surprising that AUC is different. But it isn’t wildly different which is informative about how the stable model is.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "modeling-01.html",
    "href": "modeling-01.html",
    "title": "Basic modeling",
    "section": "",
    "text": "So at this point we have point data for observation and background that have been joined with common environmental covariates (aka predictors). Here we show the basic steps taken to prepare, build and assess a model. Later, we’ll try more sophisticated modeling, such as modeling by month or splitting the data into training-testing groups."
  },
  {
    "objectID": "modeling-01.html#load-data",
    "href": "modeling-01.html#load-data",
    "title": "Basic modeling",
    "section": "1 Load data",
    "text": "1 Load data\nHere we load the observation and background data points. We add a column identifying the month of the year.\n\nsource(\"setup.R\")\n\nobs = sf::read_sf(file.path(\"data\", \"obs\", \"obs-covariates.gpkg\")) |&gt;\n  sf::st_set_geometry(\"geometry\") |&gt;\n  dplyr::mutate(month = factor(format(month_id, \"%b\"), levels = month.abb), \n                .before = geometry)\n\nbkg = sf::read_sf(file.path(\"data\", \"bkg\", \"bkg-covariates.gpkg\")) |&gt;\n  sf::st_set_geometry(\"geometry\") |&gt;\n  dplyr::mutate(month = factor(format(month_id, \"%b\"), levels = month.abb), \n                .before = geometry)"
  },
  {
    "objectID": "modeling-01.html#prepare-the-input-data",
    "href": "modeling-01.html#prepare-the-input-data",
    "title": "Basic modeling",
    "section": "2 Prepare the input data",
    "text": "2 Prepare the input data\nThe input data must be formed as two parts:\n\na plain (non-spatial) table of covariates for both observation and background free of missing data\na vector indicating which rows in the table are observations and which are background\n\n\n2.1 The input table\nSimply strip the spatial information off of obs and bkg, select just the environmental covariates, and then row bind them together\n\ninput_obs = obs |&gt;\n  sf::st_drop_geometry() |&gt; \n  dplyr::select(dplyr::all_of(c(\"sst\", \"u_wind\", \"v_wind\"))) |&gt;\n  na.omit(s)\n\ninput_bkg = bkg |&gt;\n  sf::st_drop_geometry() |&gt; \n  dplyr::select(dplyr::all_of(c(\"sst\", \"u_wind\", \"v_wind\"))) |&gt;\n  na.omit()\n\ninput_table = dplyr::bind_rows(input_obs, input_bkg)\n\n\n\n2.2 The input vector\nThe each element of the input vector must have a 1 for each observation row, and a 0 for each background row. Since we arranged to have all of the the observations come first, we can easily make the vector with two calls to rep().\n\ninput_vector = c( rep(1, nrow(input_obs)), rep(0, nrow(input_bkg)) )"
  },
  {
    "objectID": "modeling-01.html#build-the-model",
    "href": "modeling-01.html#build-the-model",
    "title": "Basic modeling",
    "section": "3 Build the model",
    "text": "3 Build the model\nHere we pass our inputs to the maxnet() function, leaving all of the optional arguments to the default values. Be sure to look over the docs for model construction - try ?maxnet\n\nmodel = maxnet::maxnet(input_vector, input_table)\n\nThat’s it. The returned object is of maxnet class; it’s essentially a list with all of the pertinent information required for subsequent use."
  },
  {
    "objectID": "modeling-01.html#assess-the-model",
    "href": "modeling-01.html#assess-the-model",
    "title": "Basic modeling",
    "section": "4 Assess the model",
    "text": "4 Assess the model\nSo what do we know about the model? Is it any good?\nOne thing we can do is to plot what are called response curves. These show, for each parameter, how the model responds along the typical range of parameter values. We plot below the responses with type cloglog which transform the response value into the 0-1 range.\n\nplot(model, type = \"cloglog\")\n\n\n\n\nLet’s take a closer look starting with sst. The model has peak response to sst in the range (approximately) 12C-17C. u_wind shows peak model response in near calm (&lt;5m/s) speeds, but interestingly eastward winds (positive) are more favorable for observation than westward winds. The north-south component of wind, v_wind seems to have strong discriminating power for northward winds (positive).\nWe can do more if we make a prediction, but, first, let’s save the model to file so we can retrieve it later."
  },
  {
    "objectID": "modeling-01.html#save-the-model",
    "href": "modeling-01.html#save-the-model",
    "title": "Basic modeling",
    "section": "5 Save the model",
    "text": "5 Save the model\nIn this tutorial we are going to build three types of models: basic, monthly and split (between testing and training). We should organize the storage of the models in a way that makes sense. With each model we may generate one or more predictions - for example, for our basic model we might tyr to hind-cast a individual years. That’s a one-to-many to relationship between model and predictions. We suggest that you start considering each model a version and store them accordingly. Let’s use a simple numbering scheme…\n\nv1.0 for the basic model\nv2.jan, v2.feb, ..., v2.dec for the monthly models\nv3.0, ... for for the split model(s)\n\nThe maxnetic provides some convenience functions for working with maxnet models including file storage functions.\n\nv1_path = file.path(\"data\", \"model\", \"v1\", \"v1.0\")\nok = dir.create(v1_path, recursive = TRUE, showWarnings = FALSE)\nmaxnetic::write_maxnet(model, file.path(v1_path, \"model_v1.0.rds\"))"
  },
  {
    "objectID": "modeling-01.html#make-a-prediction",
    "href": "modeling-01.html#make-a-prediction",
    "title": "Basic modeling",
    "section": "6 Make a prediction",
    "text": "6 Make a prediction\nNow we can make predictions with our basic model. We’ll do it two ways. First by simply feeding the input data used to create the model into the prediction. This might seems a bit circular, but it is perfectly reasonable to see how the model does on already labeled data. Second we’ll make a prediction for each month in 2020 using raster data.\n\n6.1 Predict with a data frame\nHere we provide a data frame, in our case the original input data, to the predict() function with type cloglog which transform the response value into the 0-1 range. In the histogram of predicted values below, we can see that most predicted values indicate low likelihood of observation. It’s hard to know without context what that means about our model. Further investigation is required.\n\nprediction = predict(model, input_table, type = 'cloglog')\nhist(prediction, xlab = \"prediction value\", main = \"Basic Model\")\n\n\n\n\n\n6.1.1 How did it do?\nWe can use some utilities in the maxnetic package to help us assess the model. The pAUC() function will compute statistics, include a presence-only AUC value. We need to pass it two items - the universe of predictions and the predictions for just the presence points. The plot below shows the Receiver Operator Curve (ROC) and the associated presence-only AUC value.\n\nix = input_vector &gt; 0\npauc = maxnetic::pAUC(prediction, prediction[ix])\nplot(pauc, title = \"v1.0 Basic Model\")\n\n\n\n\nOverall, this is telling us that the model isn’t especially strong as a prediction tool, but it is much better than a 50-50 guess (that’s when AUC is close to 0.5, and the curve follows the light grey line). Learn more about ROC and AUC here.\n\n\n\n6.2 Predict with rasters\nWe can also predict using raster inputs using our basic model. Let’s read in rasters for each month of 2019, and then run a prediction for each month.\nWe provide a function read_predictors() that will read and bind the rasters together for you given the filtered databases and paths. So, first we define the paths and filter the databases to point to just the months in 2019.\n\ndates = as.Date(c(\"2019-01-01\", \"2019-12-31\"))\n\nsst_path = \"data/oisst\"\nsst_db = oisster::read_database(sst_path) |&gt;\n  dplyr::arrange(date) |&gt;\n  dplyr::filter(dplyr::between(date, dates[1], dates[2]))\n\nwind_path = \"data/nbs\"\nwind_db = nbs::read_database(wind_path) |&gt;\n  dplyr::arrange(date)|&gt;\n  dplyr::filter(dplyr::between(date, dates[1], dates[2]))\n\nu_wind_db = wind_db |&gt;\n  dplyr::filter(param == \"u_wind\")|&gt;\n  dplyr::filter(dplyr::between(date, dates[1], dates[2]))\n\nv_wind_db = wind_db |&gt;\n  dplyr::filter(param == \"v_wind\")|&gt;\n  dplyr::filter(dplyr::between(date, dates[1], dates[2]))\n\npredictors = read_predictors(sst_db = sst_db,\n                             u_wind_db = u_wind_db,\n                             v_wind_db = v_wind_db)\npredictors\n\nstars object with 3 dimensions and 3 attributes\nattribute(s):\n             Min.   1st Qu.     Median       Mean   3rd Qu.      Max. NA's\nsst     -1.558928 12.775290 19.6539297 17.7602351 23.599515 29.367096 9840\nu_wind  -2.692028  1.144244  2.7007004  2.7228278  4.115177 13.148120 7612\nv_wind  -5.431324 -1.411349 -0.3202622 -0.1398384  1.106175  4.636874 7612\ndimension(s):\n     from to offset delta refsys point                    values x/y\nx       1 74 -76.38  0.25 WGS 84 FALSE                      NULL [x]\ny       1 46  46.12 -0.25 WGS 84 FALSE                      NULL [y]\ntime    1 12     NA    NA   Date    NA 2019-01-01,...,2019-12-01    \n\n\nYou can see that we have the rasters in one object of three attributes (sst, u_wind and v_wind) each with 12 layers (Jan 2019 - Dec 2019). Now we can run the prediction.\n\npred = predict(model, predictors, type = 'cloglog')\npred\n\nstars object with 3 dimensions and 1 attribute\nattribute(s):\n              Min.    1st Qu.    Median      Mean   3rd Qu.      Max. NA's\npred  3.836567e-06 0.06545853 0.1585141 0.2314229 0.3466397 0.9540259 9949\ndimension(s):\n     from to offset delta refsys point                    values x/y\nx       1 74 -76.38  0.25 WGS 84 FALSE                      NULL [x]\ny       1 46  46.12 -0.25 WGS 84 FALSE                      NULL [y]\ntime    1 12     NA    NA   Date    NA 2019-01-01,...,2019-12-01    \n\n\nSince we get a spatially mapped prediction back, we can plot it.\n\ncoast = rnaturalearth::ne_coastline(scale = 'large', returnclass = 'sf') |&gt;\n  sf::st_crop(pred)\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nplot_coast = function() {\n  plot(sf::st_geometry(coast), col = 'green', add = TRUE)\n}\nplot(pred, hook = plot_coast)\n\n\n\n\nWell, that certainly looks appealing with higher likelihood of near shore observations occurring during the warmer months.\n\n6.2.1 How did it do?\nTo compute an ROC and AUC for each month, we have a little bit of work to do. We need to extract the observations locations for each month from the prediction maps. These we can then plot.\n\n\n\n\n\n\nNote\n\n\n\nWe have to modify the date for each point to be the first date of each month. That’s because our predictors are monthlies.\n\n\n\ntest_obs = obs |&gt;\n  dplyr::filter(dplyr::between(date, dates[1], dates[2])) |&gt;\n  dplyr::select(dplyr::all_of(\"date\")) |&gt;\n  dplyr::mutate(date = oisster::current_month(date))\n\nx = stars::st_extract(pred, test_obs, time_column = 'date') |&gt;\n  print()\n\nSimple feature collection with 612 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -75.7589 ymin: 35.1211 xmax: -65.48274 ymax: 43.83954\nGeodetic CRS:  WGS 84\nFirst 10 features:\n        pred       time       date                   geometry\n1  0.2815698 2019-05-01 2019-05-01 POINT (-67.32935 40.42509)\n2  0.5092503 2019-03-01 2019-03-01 POINT (-74.41057 36.49908)\n3  0.3727513 2019-12-01 2019-12-01   POINT (-75.3994 35.9457)\n4  0.5328261 2019-06-01 2019-06-01 POINT (-75.10864 36.94806)\n5  0.5999803 2019-04-01 2019-04-01 POINT (-74.49892 36.57275)\n6  0.1139667 2019-09-01 2019-09-01   POINT (-75.5519 36.1854)\n7  0.5478690 2019-09-01 2019-09-01   POINT (-73.6245 40.3317)\n8  0.6070690 2019-04-01 2019-04-01 POINT (-69.04389 39.82132)\n9  0.8075227 2019-04-01 2019-04-01 POINT (-74.59436 36.87291)\n10 0.8075227 2019-04-01 2019-04-01 POINT (-74.45753 36.72279)\n\n\nFinally we can build a table that merges the prediction with the labels. We are going to add the name of the month to group by that.\n\ny = x |&gt;\n  dplyr::mutate(month = factor(format(date, \"%b\"), levels = month.abb), \n                .before = 1) |&gt;\n  dplyr::select(dplyr::all_of(c(\"month\", \"pred\", \"date\"))) |&gt;\n  dplyr::group_by(month) \n\ndplyr::count(y, month) |&gt;\n  print(n = 12)\n\nSimple feature collection with 12 features and 2 fields\nGeometry type: MULTIPOINT\nDimension:     XY\nBounding box:  xmin: -75.7589 ymin: 35.1211 xmax: -65.48274 ymax: 43.83954\nGeodetic CRS:  WGS 84\n# A tibble: 12 × 3\n   month     n                                                          geometry\n * &lt;fct&gt; &lt;int&gt;                                                  &lt;MULTIPOINT [°]&gt;\n 1 Jan      21 ((-74.63902 36.26849), (-75.01758 36.49984), (-75.01801 36.72554…\n 2 Feb       7 ((-74.52432 37.24967), (-74.45561 37.16891), (-74.74373 36.72355…\n 3 Mar      23 ((-74.53117 36.26996), (-74.60195 36.72201), (-74.67127 36.72266…\n 4 Apr     169 ((-72.924 38.6733), (-73.0165 38.591), (-73.0036 38.56), (-73.10…\n 5 May     119 ((-74.56571 35.6059), (-75.2181 35.1934), (-75.3228 35.535), (-7…\n 6 Jun      53 ((-73.10608 38.72575), (-74.86204 36.27105), (-75.04656 36.34824…\n 7 Jul      48 ((-74.53554 36.19828), (-74.91756 36.27104), (-75.10905 36.27065…\n 8 Aug      39 ((-72.78628 38.68677), (-72.98868 38.61241), (-74.9889 36.2911),…\n 9 Sep      21 ((-75.3167 36.0439), (-75.5204 36.3294), (-75.5519 36.1854), (-7…\n10 Oct      79 ((-67.06445 42.91399), (-68.43614 43.83954), (-69.14391 43.16967…\n11 Nov      19 ((-72.52681 39.21286), (-71.54966 39.99385), (-67.79606 40.36107…\n12 Dec      14 ((-75.242 35.2705), (-75.3335 35.3027), (-75.436 35.1211), (-75.…\n\n\nNow how about one ROC plot for each month? Yikes! This requires a iterative approach, using group_map(), to compute the ROC for each month. We then follow with plot wrapping by the patchwork package.\n\npaucs = dplyr::group_map(y, \n  function(tbl, key, pred_rasters = NULL){\n    ix = key$month %in% month.abb\n    x = dplyr::slice(pred_rasters, \"time\", ix)\n    pauc = maxnetic::pAUC(x, tbl)\n    plot(pauc,title = key$month, \n         xlab = \"\", ylab = \"\")\n  }, pred_rasters = pred)\n\npatchwork::wrap_plots(paucs, ncol = 4)\n\n\n\n\nHmmm. That’s surprising, yes? Why during the summer months does our AUC go down when we have the most number of observations? That seems counter intuitive."
  },
  {
    "objectID": "modeling-01.html#thinking-about-auc",
    "href": "modeling-01.html#thinking-about-auc",
    "title": "Basic modeling",
    "section": "7 Thinking about AUC",
    "text": "7 Thinking about AUC\nAUC is a diagnostic that provides a peek into the predictive power of a model. But what is it? An analogy is fitting a straight line to a small set of observations verses a large set of observations and then comparing the correlation coefficients. Here’s a simple example using R’s built-in dataset cars which is a data frame of 50 observations of speed and stopping distances of cars. We’ll compute a linear model for the entire data set, and then a second for a small subsample of the data. (Learn more about linear models in R here.)\n\ndata(\"cars\")\ncars = dplyr::as_tibble(cars)\n\nall_fit = lm(dist ~ speed, data = cars)\nsummary(all_fit)\n\n\nCall:\nlm(formula = dist ~ speed, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nspeed         3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\n\nset.seed(5)\nsub_cars = dplyr::slice(cars, c(10, 28, 50))\nsub_fit = lm(dist ~ speed, data = sub_cars)\nsummary(sub_fit)\n\n\nCall:\nlm(formula = dist ~ speed, data = sub_cars)\n\nResiduals:\n      1       2       3 \n 0.5364 -0.8344  0.2980 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -37.1523     1.8867  -19.69   0.0323 *\nspeed         4.8742     0.1032   47.21   0.0135 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.036 on 1 degrees of freedom\nMultiple R-squared:  0.9996,    Adjusted R-squared:  0.9991 \nF-statistic:  2229 on 1 and 1 DF,  p-value: 0.01348\n\n\nYou can see that the rU+00B2 value is quite high for the smaller data set, but the model may not be predictive over the full range of data. AUC is somewhat analogous to to rU+00B2 in that a relatively low score does not necessarily suggest a poor model.\n\nggplot2::ggplot(data = cars, ggplot2::aes(x = speed, y = dist)) +\n  ggplot2::geom_point(color = \"blue\") +\n  ggplot2::geom_abline(slope = coef(all_fit)[2], intercept = coef(all_fit)[1], color = \"blue\") + \n  ggplot2::geom_point(data = sub_cars, ggplot2::aes(x = speed, y = dist), \n                      color = \"orange\", shape = 1, size = 3) +\n  ggplot2::geom_abline(slope = coef(sub_fit)[2], intercept = coef(sub_fit)[1], color = \"orange\")"
  },
  {
    "objectID": "predictors.html",
    "href": "predictors.html",
    "title": "Predictors",
    "section": "",
    "text": "Predictor variables (aka “covariate variables” and “environmental variables”) are provided in the downloaded observations. We can certainly use that data, but to use MaxEnt presence only modeling, we need a way to characterize the background. For that we select random points from the same region and times that we have observations. Well, those data do not come with the OBIS download, we have to get them ourselves.\nAnother nice thing about using external predictor variables, is that we can use them, once we have a model in hand, to make predictive maps."
  },
  {
    "objectID": "predictors.html#why-predictor-variables",
    "href": "predictors.html#why-predictor-variables",
    "title": "Predictors",
    "section": "",
    "text": "Predictor variables (aka “covariate variables” and “environmental variables”) are provided in the downloaded observations. We can certainly use that data, but to use MaxEnt presence only modeling, we need a way to characterize the background. For that we select random points from the same region and times that we have observations. Well, those data do not come with the OBIS download, we have to get them ourselves.\nAnother nice thing about using external predictor variables, is that we can use them, once we have a model in hand, to make predictive maps."
  },
  {
    "objectID": "predictors.html#what-predictor-variables",
    "href": "predictors.html#what-predictor-variables",
    "title": "Predictors",
    "section": "2 What predictor variables?",
    "text": "2 What predictor variables?\nHere we chose to use monthly means of sea surface temperature OISST and sea winds NBS v2.\nEach of these are available as easily access on-line monthly datasets. Monthly means seem like a good granularity for this dataset (and a tutorial). It is appealing, but unsubstantiated, that SST and winds would have an influence on Mola mola/observer interactions."
  },
  {
    "objectID": "predictors.html#storing-large-data-and-github",
    "href": "predictors.html#storing-large-data-and-github",
    "title": "Predictors",
    "section": "3 Storing large data and github",
    "text": "3 Storing large data and github\nLet’s make two sub-directories in our data directory; one each for OISST and NBS2.\n\nsource(\"setup.R\")\nok = dir.create(\"data/oisst\", recursive = TRUE, showWarnings = FALSE)\nok = dir.create(\"data/nbs2\", recursive = TRUE, showWarnings = FALSE)\n\nIt is not unreasonable to keep your data in your project directory, but gridded data can be quite large. Sometimes it gets so large that the size limit on github repositories can be exceeded. The simplest way to prevent that is to prevent git from tracking your large data directories by adding filters to your local .gitignore file. Here’s what we put in our project’s .gitignore file. These lines tell git to not catalog items found in those two directories.\n# Biggish data directories\ndata/oisst/*\ndata/nbs2/*"
  },
  {
    "objectID": "predictors.html#fetching-data",
    "href": "predictors.html#fetching-data",
    "title": "Predictors",
    "section": "4 Fetching data",
    "text": "4 Fetching data\n\n4.1 OISST sea surface temperature data\nWe’ll use the oisster R package to fetch OISST monthly data. Now, OISST is mapped into the longitude range of [0,360], but our own data is mapped into the longitude range [-180,180]. So we need to request a box that fits into OISST’s range. The oisster R package) provides a function to help translate the bounding box. This can a little while.\nAt the end we build a small database of the files we have saved, which will be very helpful to us later when we need to find and open the files. the oisster package provides a number of organizational utilities, such as saving data into a directory hierarchy that works well when storing a mix of data (monthlies, dailies, etc). We don’t really leverage these utilities a lot, but what is provided is helpful to us.\nFirst, let’s getthe bounding box and transfprm it to a 0-360 longitude range.\n\nbb = get_bb(form = \"numeric\")\nbb = oisster::to_360_bb(bb)\n\nAnd now get the rasters. This can take a long time, so we test for the existence of the local NBS database first. If the database doesn’t yet exist, then we download the data.\n\noisst_path = \"data/oisst\"\noisst_db_file = file.path(oisst_path, \"database.csv.gz\")\n\nif (file.exists(oisst_db_file)){\n  sst_db = oisster::read_database(oisst_path)\n} else {\n  sst = oisster::fetch_month(dates = seq(from = as.Date(\"2000-01-01\"),\n                                       to = current_month(offset = -1),\n                                       by = \"month\"),\n                           bb = bb,\n                           path = oisst_path)\n  sst_db = oisster::build_database(oisst_path, save_db = TRUE)\n}\nsst_db\n\n# A tibble: 285 × 5\n   date       param per   trt   ltm  \n   &lt;date&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt;\n 1 2000-01-01 sst   mon   mean  NA   \n 2 2000-02-01 sst   mon   mean  NA   \n 3 2000-03-01 sst   mon   mean  NA   \n 4 2000-04-01 sst   mon   mean  NA   \n 5 2000-05-01 sst   mon   mean  NA   \n 6 2000-06-01 sst   mon   mean  NA   \n 7 2000-07-01 sst   mon   mean  NA   \n 8 2000-08-01 sst   mon   mean  NA   \n 9 2000-09-01 sst   mon   mean  NA   \n10 2000-10-01 sst   mon   mean  NA   \n# ℹ 275 more rows\n\n\nThe ltm is reserved for use with the long-term-mean products which we do not download. We can select from the database a small number of file to load and view.\n\n# first we filter\nsub_sst_db = sst_db |&gt;\n  dplyr::filter(dplyr::between(date, as.Date(\"2000-01-01\"), as.Date(\"2000-12-01\")))\n\n# then read into a multi=layer stars object\nsst = stars::read_stars(oisster::compose_filename(sub_sst_db, oisst_path),\n                        along = list(date = sub_sst_db$date))\nplot(sst)\n\n\n\n\n\n\n4.2 NBS wind data\nFormerly known as “Blended Sea Winds”, the NBS v2 data set provides global wind estimates of wind (u and v components as well as windspeed) gong back to 1987. We shall collect monthly raster for our study area from 2000 to close to present (nbs lags by about 2 months). Like OISST, NBS is served on a longitude range of 0-360.\n\n\n\n\n\n\nNote\n\n\n\nThe NBS dataset for monthly UV has some issues - certain months are not present even though the server says they are. This may generate error messages which we can ignore, but the data for that month will not be downloaded.\n\n\nThis can take a long time, so we test for the existence of the local NBS database first. If the database doesn’t yet exist, then we download the data.\n\nwind_path = 'data/nbs'\nwind_db_file = file.path(wind_path, \"database.csv.gz\")\n\nif (file.exists(wind_db_file)){\n  wind_db = nbs::read_database(wind_path)\n} else {\n  wind = nbs::fetch_month(dates = seq(from = as.Date(\"2000-01-01\"),\n                                       to = Sys.Date() - 75,\n                                       by = \"month\"),\n                           bb = bb,\n                           path = wind_path)\n  wind_db = nbs::build_database(wind_path, save_db = TRUE)\n}\n\nJust as with sst, we can select from the database a small number of file to load and view.\n\n# first we filter\nsub_wind_db = wind_db |&gt;\n  dplyr::filter(dplyr::between(date, as.Date(\"2000-01-01\"), as.Date(\"2000-12-01\")),\n                param == \"windspeed\")\n\n\n# then read into a multi=layer stars object\nwind = stars::read_stars(nbs::compose_filename(sub_wind_db, wind_path),\n                        along = list(date = sub_wind_db$date)) |&gt;\n  rlang::set_names(\"windspeed\")\nplot(wind)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "OpenCage, CC BY-SA 2.5, via Wikimedia Commons"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Home",
    "section": "1 Overview",
    "text": "1 Overview\nHere we present a possible workflow for modeling and predicting human interactions with the world’s largest bony fish, the Mola mola. We’ll use R and a number of important packages to retrieve observations from OBIS, sea surface temperature from OISST and wind estimates from NBS v2. With these we’ll build a presence-only model using a pure-R implementation of MaxEnt. We’ll also try our hand at predicting (hindcast).\nThe cartoon below generalizes the two-step process that we’ll walk through. The first step is where most the coding efforts occurs as we gather our observations and predictor variables we have chosen. The product of this first step, the model, is often saved to disk for later use making predictions.\n\n\n\noverview"
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Home",
    "section": "2 Prerequisites",
    "text": "2 Prerequisites\nWe are making an assumption that the reader is familiar with R programming language, the tools available in the tidyverse, and spatial data handling."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Home",
    "section": "3 Getting started",
    "text": "3 Getting started\n\n“According to the ancient Chinese proverb, A journey of a thousand miles must begin with a single step.” ~ John F. Kennedy\n\n\n3.1 Handling spatial data\nIn this tutorial we use the sf and stars packages to handle spatial data. These tutorials for sf and for stars will help you off to a great start.\n\n\n3.2 Project-specific functions\nWe have developed a suite of functions that facilitate accessing and working with data. These can be loaded into your R session by source()-ing the setup.R file. Here’s an example where we show the study area using the ancillary function get_bb() to retrieve the project’s bounding box.\n\nsource(\"setup.R\", echo = FALSE)\nbb = get_bb(form = 'polygon')\ncoast = rnaturalearth::ne_coastline(scale = 'large', returnclass = 'sf')\nplot(sf::st_geometry(coast), extent = bb, axes = TRUE, reset = FALSE)\nplot(bb, lwd = 2, border = 'orange', add = TRUE)\n\n\n\n\nThe setup file also checks for the required packages, and will attempt to install them into the user’s R library directory if not already installed.\n\n\n3.3 Fetching data\nThe robis package facilitates easy access to OBIS which is a huge public database for oceanographic species information. We have written a wrapper function to download the Mola mola species records in our study region. To simplify our task we drop may columns of data from that delivered by OBIS, but there is much in the original worth exploring. Note you can use this function to access other species in other parts of the world.\n\n\n\n\n\n\nNote\n\n\n\nNote that we have already fetched the data, so we don’t run this next step (but you can if you like to get updated data.)\n\n\n\nif (!file.exists('data/obis/Mola_mola.gpkg')){\n  x = fetch_obis(scientificname = 'Mola mola')\n}\n\nSince we already have the data, we need only read it which we do below. We also “glimpse” at the data so we can get a sense of the variables within the data frame and their respective data types.\n\nx = read_obis(scientificname = 'Mola mola') |&gt;\n  dplyr::glimpse()\n\nRows: 10,646\nColumns: 8\n$ occurrenceID  &lt;chr&gt; \"1817_15415\", \"283_15788\", \"513_6751\", \"513_49055\", \"513…\n$ date          &lt;date&gt; 2016-08-07, 1980-05-23, 2002-06-17, 2009-07-10, 2009-08…\n$ basisOfRecord &lt;chr&gt; \"HumanObservation\", \"HumanObservation\", \"HumanObservatio…\n$ bathymetry    &lt;dbl&gt; 121, 102, 65, 166, 133, 19, 218, 90, 1283, 160, 219, 97,…\n$ shoredistance &lt;dbl&gt; 134559, 121292, 77684, 127643, 58028, 524, 94816, 141127…\n$ sst           &lt;dbl&gt; 16.36, 15.44, 13.38, 11.70, 9.20, 12.35, 11.79, 13.90, 1…\n$ sss           &lt;dbl&gt; 34.10, 33.41, 32.64, 32.22, 30.99, 31.38, 32.12, 33.38, …\n$ geom          &lt;POINT [°]&gt; POINT (-72.8074 39.056), POINT (-72.283 39.733), P…\n\n\nLet’s see what we found on a map. We first plot the coastline, but provide the bounding box to establish the limits of the plot. Then we add the box itself and then the points.\n\nplot(sf::st_geometry(coast), extent = bb, axes = TRUE, reset = FALSE)\nplot(bb, lwd = 2, border = 'orange', add = TRUE)\nplot(sf::st_geometry(x), pch = \"+\", col = 'blue', add = TRUE)\n\n\n\n\n\n\n3.4 Data storage\nWe have set up a data directory, data, for storing data collected for the project. To start out there isn’t much more than the downloaded data set, but we’ll added to it as we go. We try to keep the data storage area tidy by using subdirectories. Below we print the directory tree, your’s might look slightly different until you have run this code.\n\nfs::dir_tree(\"data\", recurse = 1)\n\ndata\n├── bkg\n│   ├── bkg-covariates.gpkg\n│   └── buffered-polygon.gpkg\n├── model\n│   ├── v1\n│   ├── v2\n│   └── v3\n├── nbs2\n│   ├── 2000\n│   ├── 2001\n│   ├── 2002\n│   ├── 2003\n│   ├── 2004\n│   ├── 2005\n│   ├── 2006\n│   ├── 2007\n│   ├── 2008\n│   ├── 2009\n│   ├── 2010\n│   ├── 2011\n│   ├── 2012\n│   ├── 2013\n│   ├── 2014\n│   ├── 2015\n│   ├── 2016\n│   ├── 2017\n│   ├── 2018\n│   ├── 2019\n│   ├── 2020\n│   ├── 2021\n│   ├── 2022\n│   ├── 2023\n│   └── database.csv.gz\n├── obis\n│   └── Mola_mola.gpkg\n├── obs\n│   ├── obs-covariates.gpkg\n│   └── obs.gpkg\n└── oisst\n    ├── 2000\n    ├── 2001\n    ├── 2002\n    ├── 2003\n    ├── 2004\n    ├── 2005\n    ├── 2006\n    ├── 2007\n    ├── 2008\n    ├── 2009\n    ├── 2010\n    ├── 2011\n    ├── 2012\n    ├── 2013\n    ├── 2014\n    ├── 2015\n    ├── 2016\n    ├── 2017\n    ├── 2018\n    ├── 2019\n    ├── 2020\n    ├── 2021\n    ├── 2022\n    ├── 2023\n    └── database.csv.gz"
  },
  {
    "objectID": "observations.html",
    "href": "observations.html",
    "title": "Getting to know your observations",
    "section": "",
    "text": "It is important to be well versed with your observation data much as a chef knows ingredients. Let’s start by reading in the observations and making some simple counts and plots.\nsource(\"setup.R\", echo = FALSE)\nx = read_obis()\nLets start by counting the various records that make up basisOfRecord. In this case, we are not interested in the spatial location of the observations so we drop the spatial info (which saves time during the counting process.)\nsf::st_drop_geometry(x) |&gt;\n  dplyr::count(basisOfRecord)\n\n# A tibble: 1 × 2\n  basisOfRecord        n\n  &lt;chr&gt;            &lt;int&gt;\n1 HumanObservation 10646\nSo, all are from human observation (not machine based observations or museum specimen)."
  },
  {
    "objectID": "observations.html#examining-embedded-covariates",
    "href": "observations.html#examining-embedded-covariates",
    "title": "Getting to know your observations",
    "section": "1 Examining embedded covariates",
    "text": "1 Examining embedded covariates\nCovariates are those variables that we can use to model Mola mola observations, and, to an extent the distribution, of Mola mola themselves. Some covariates come with the OBIS download - such as surface temperature, surface salinity, distance to the shore and bathymetric depth. Let’s explore these; first we make a 2d histogram of sst and sss.\n\nggplot2::ggplot(x, ggplot2::aes(x=sst, y=sss) ) +\n  ggplot2::geom_bin2d(bins = 60) +\n  ggplot2::scale_fill_continuous(type = \"viridis\") \n\nWarning: Removed 1 rows containing non-finite values (`stat_bin2d()`).\n\n\n\n\n\nIt looks like there is some confluence of SSS-SST and when observations occur. It will be interesting the see how that plays out in our modeling.\nLet’s do the same with bathymetry and shoredistance.\n\nggplot2::ggplot(x, ggplot2::aes(x = shoredistance, y = bathymetry) ) +\n  ggplot2::geom_bin2d(bins = 60) +\n  ggplot2::scale_fill_continuous(type = \"viridis\")\n\n\n\n\nHmmm. This makes sense, that most are observed near shore where the depths are relatively shallow. But some are found for offshore in deep waters. So, does this reflect an observer bias? Or does this reflect a behavior on the part of Mola mola?"
  },
  {
    "objectID": "observations.html#observations-through-time",
    "href": "observations.html#observations-through-time",
    "title": "Getting to know your observations",
    "section": "2 Observations through time",
    "text": "2 Observations through time\nLet’s add year and month columns and make a 2d-histogram of those.\n\nx = dplyr::mutate(x, year = as.integer(format(date, \"%Y\")), \n           month = factor(format(date, \"%b\"), levels = month.abb))\nggplot2::ggplot(x, ggplot2::aes(x=month, y=year) ) +\n  ggplot2::geom_bin2d() +\n  ggplot2::scale_fill_continuous(type = \"viridis\") +\n  ggplot2::geom_hline(yintercept = 2000, color = 'orange', linewidth = 1)\n\n\n\n\nNot too surprisingly, most observation are during warmer months. And it looks like most occur from 2000s onward (orange line) which is convenient if we want to leverage satellite data into our suite of predictive covariates.\nHow do these look spatially?\n\nbb = get_bb(form = 'polygon')\ncoast = rnaturalearth::ne_coastline(scale = 'large', returnclass = 'sf') |&gt;\n  st_crop(bb)\n\nggplot(x) +\n  geom_sf(data = x, color = \"blue\", alpha = 0.3, shape = \"bullet\") +\n  geom_sf(data = coast) +\n  facet_wrap(~ month)\n\n\n\n\nIt seems that either the Mola mola vacate the Gulf of Maine in winter, or observers in the Gulf of Maine stop reporting. In either case, the observations in winter months are very low compared to summer."
  },
  {
    "objectID": "covariates.html",
    "href": "covariates.html",
    "title": "Covariates for observations and background",
    "section": "",
    "text": "Here we do the multi-step task of associating observations with environmental covariates and creating a background point data set used by the model to characterize the environment."
  },
  {
    "objectID": "covariates.html#background-points-to-characterize-the-environment",
    "href": "covariates.html#background-points-to-characterize-the-environment",
    "title": "Covariates for observations and background",
    "section": "1 Background points to characterize the environment",
    "text": "1 Background points to characterize the environment\nPresence only modeling using MaxEnt or the pure R implementation maxnet require that a sample representing “background” is provided for the model building stage. Background points are used to characterize the environment in which the presence points are found, the modeling algorithm uses that information to discriminate between suitability and unsuitability of the environment. It is good practice to sample “background” in the same spatial and temporal range as the presence data. That means we need to define a bounding polygon around the presence locations from which we can sample, as well as sampling through time."
  },
  {
    "objectID": "covariates.html#loading-data",
    "href": "covariates.html#loading-data",
    "title": "Covariates for observations and background",
    "section": "2 Loading data",
    "text": "2 Loading data\nWe have two data sources to load: point observation data and rasterized environmental predictor data.\n\n2.1 Load the observation data\nWe’ll load in our OBIS observations as a flat table, and immediately filter the data to any occurring from 2000 to present.\n\nsource(\"setup.R\")\n\nobs = read_obis(form = \"sf\") |&gt;\n  dplyr::filter(date &gt;= as.Date(\"2000-01-01\")) |&gt;\n  dplyr::glimpse()\n\nRows: 8,561\nColumns: 8\n$ occurrenceID  &lt;chr&gt; \"1817_15415\", \"513_6751\", \"513_49055\", \"513_49875\", \"513…\n$ date          &lt;date&gt; 2016-08-07, 2002-06-17, 2009-07-10, 2009-08-17, 2016-06…\n$ basisOfRecord &lt;chr&gt; \"HumanObservation\", \"HumanObservation\", \"HumanObservatio…\n$ bathymetry    &lt;dbl&gt; 121, 65, 166, 133, 218, 90, 1283, 160, 219, 97, 131, 97,…\n$ shoredistance &lt;dbl&gt; 134559, 77684, 127643, 58028, 94816, 141127, 114100, 382…\n$ sst           &lt;dbl&gt; 16.36, 13.38, 11.70, 9.20, 11.79, 13.90, 18.34, 9.08, 11…\n$ sss           &lt;dbl&gt; 34.10, 32.64, 32.22, 30.99, 32.12, 33.38, 34.96, 30.47, …\n$ geom          &lt;POINT [°]&gt; POINT (-72.8074 39.056), POINT (-70.5774 40.5725),…\n\n\n\n\n2.2 Load the environmental predictors\nNext we load the environmental predictors, sst and wind (as windspeed, u_wind and v_wind). For each we first read in the database, then compose filenames, read in the rasters and organize into layers by date, then rename to a pretty-name and finally shift the rasters from their native 0-to-360 longitude range to longitude -180-to-180 which matches our observations.\n\nsst_path = \"data/oisst\"\nsst_db = oisster::read_database(sst_path) |&gt;\n  dplyr::arrange(date)\n\nsst = sst_db |&gt;\n  oisster::compose_filename(path = sst_path) |&gt;\n  stars::read_stars(along = list(time = sst_db$date)) |&gt;\n  rlang::set_names(\"sst\")|&gt;\n  st_to_180()\n\n\nwind_path = \"data/nbs\"\nwind_db = nbs::read_database(wind_path) |&gt;\n  dplyr::arrange(date)\n\nwindspeed_db = wind_db |&gt;\n  dplyr::filter(param == \"windspeed\")\nwindspeed = windspeed_db |&gt;\n  nbs::compose_filename(path = wind_path) |&gt;\n  stars::read_stars(along = list(time = windspeed_db$date)) |&gt;\n  rlang::set_names(\"windspeed\") |&gt;\n  st_to_180()\n\nu_wind_db = wind_db |&gt;\n  dplyr::filter(param == \"u_wind\")\nu_wind = u_wind_db |&gt;\n  nbs::compose_filename(path = wind_path) |&gt;\n  stars::read_stars(along = list(time = u_wind_db$date)) |&gt;\n  rlang::set_names(\"u_wind\") |&gt;\n  st_to_180()\n\nv_wind_db = wind_db |&gt;\n  dplyr::filter(param == \"v_wind\")\nv_wind = v_wind_db |&gt;\n  nbs::compose_filename(path = wind_path) |&gt;\n  stars::read_stars(along = list(time = v_wind_db$date)) |&gt;\n  rlang::set_names(\"v_wind\") |&gt;\n  st_to_180()\n\nWe’ll set these aside for a moment and come back to them after we have established our background points."
  },
  {
    "objectID": "covariates.html#sampling-background-data",
    "href": "covariates.html#sampling-background-data",
    "title": "Covariates for observations and background",
    "section": "3 Sampling background data",
    "text": "3 Sampling background data\nWe need to create a random sample of background in both time and space.\n\n3.1 Sampling time\nSampling time requires us to consider that the occurrences are not evenly distributed through time. We can see that using a histogram of observation dates by month.\n\nH = hist(obs$date, breaks = 'month', format = \"%Y\", \n     freq = TRUE, main = \"Observations\",\n     xlab = \"Date\")\n\n\n\n\nWe could use weighted sampling so we are characterizing the environment consistent with the observations. But the purpose of the sampling isn’t to mimic the distrubution of observations in time, but instead to characterize the environment. So, instead we’ll make an unweighted sample across the time range. First we make a time series that extends from the first to the last observation date plus a buffer of about 1 month.\n\nn_buffer_days = 30\ndays = seq(from = min(obs$date) - n_buffer_days, \n           to = max(obs$date) + n_buffer_days, \n           by = \"day\")\n\nNow we can sample - but how many? Let’s start by selecting approximately four times as many background points as we have observation points. If it is too many then we can sub-sample as needed, if it isn’t enough we can come back an increase the number. In addition, we may lose some samples in the subsequent steps making a spatial sample.\n\n\n\n\n\n\nNote\n\n\n\nNote that we set the random number generator seed. This isn’t a requirement, but we use it here so that we get the same random selection each time we render the page. Here’s a nice discussion about set.seed() usage.\n\n\n\nset.seed(1234)\nnback = nrow(obs) * 4\ndays_sample = sample(days, size = nback, replace = TRUE)\n\nNow we can plot the same histogram, but with the days_sample data.\n\nH = hist(days_sample, breaks = 'month', format = \"%Y\", \n     freq = TRUE, main = \"Sample\",\n     xlab = \"Date\")\n\n\n\n\n\n\n3.2 Sampling space\nThe sf package provides a function, st_sample(), for sampling points within a polygon. But what polygon? We have choices as we could use (a) a bounding box around the observations, (b) a convex hull around the observations or (c) a buffered envelope around the observations. Each has it’s advantages and disadvantages. We show how to make one of each.\n\n3.2.1 The bounding box polygon\nThis is the easiest of the three polygons to make.\n\ncoast = rnaturalearth::ne_coastline(scale = 'large', returnclass = 'sf')\n\nbox = sf::st_bbox(obs) |&gt;\n  sf::st_as_sfc()\n\nplot(sf::st_geometry(coast), extent = box, axes = TRUE)\nplot(box, lwd = 2, border = 'orange', add = TRUE)\nplot(sf::st_geometry(obs), pch = \"+\", col = 'blue', add = TRUE)\n\n\n\n\nHmmm. It is easy to make, but you can see vast stretches of sampling area where no observations have been reported (including on land). That could limit the utility of the model.\n\n\n3.2.2 The convex hull polygon\nAlso an easy polygon to make is a convex hull - this is one often described as the rubber-band stretched around the point locations. The key here is to take the union of the points first which creates a single MULTIPOINT object. If you don’t you’ll get a convex hull around every point… oops.\n\nchull = sf::st_union(obs) |&gt;\n  sf::st_convex_hull()\n\nplot(sf::st_geometry(coast), extent = chull, axes = TRUE)\nplot(sf::st_geometry(chull), lwd = 2, border = 'orange', add = TRUE)\nplot(sf::st_geometry(obs), pch = \"+\", col = 'blue', add = TRUE)\n\n\n\n\nWell, that’s an improvement, but we still get large areas vacant of observations and most of Nova Scotia.\n\n\n3.2.3 The buffered polygon\nAn alternative is to create a buffered polygon around the MULTIPOINT object. We like to think of this as the “shrink-wrap” version as it follows the general contours of the points. We arrived at a buffereing distance of 75000m through trial and error, and the add in a smoothing for no other reason to improve aesthetics.\n\npoly =  sf::st_union(obs) |&gt;\n  sf::st_buffer(dist = 75000) |&gt;\n  sf::st_union() |&gt;\n  sf::st_simplify() |&gt;\n  smoothr::smooth(method = 'chaikin', refinements = 10L)\n\n\nplot(sf::st_geometry(coast), extent = poly, axes = TRUE)\nplot(sf::st_geometry(poly), lwd = 2, border = 'orange', add = TRUE)\nplot(sf::st_geometry(obs), pch = \"+\", col = 'blue', add = TRUE)\n\n\n\n\nThat seems the best yet, but we still sample on land. We’ll over sample and toss out the ones on land. Let’s save this polygon in case we need it later.\n\nok = dir.create(\"data/bkg\", recursive = TRUE, showWarnings = FALSE)\nsf::write_sf(poly, file.path(\"data\", \"bkg\", \"buffered-polygon.gpkg\"))\n\n\n\n3.2.4 Sampling the polygon\nNow to sample the within the polygon, we’ll sample the same number we selected earlier. Note that we also set the same seed (for demonstration purposes).\n\nset.seed(1234)\nbkg = sf::st_sample(poly, nback) \n\nplot(sf::st_geometry(coast), extent = poly, axes = TRUE)\nplot(sf::st_geometry(poly), lwd = 2, border = 'orange', add = TRUE)\nplot(sf::st_geometry(bkg), pch = \".\", col = 'blue', add = TRUE)\nplot(sf::st_geometry(coast), add = TRUE)\n\n\n\n\nOK - we can work with that! We still have points on land, but most are not. The following section shows how to use SST maps to filter out errant background points.\n\n\n3.2.5 Purging points that are on land (or very nearshore)\nIt’s great if you have in hand a map the distinguishes between land and sea - like we do with sst. We shall extract values v from just the first sst layer (hence the slice).\n\nv = sst |&gt;\n  dplyr::slice(along = \"time\", 1) |&gt;\n  stars::st_extract(bkg) |&gt;\n  sf::st_as_sf() |&gt;\n  dplyr::mutate(is_water = !is.na(sst), .before = 1) |&gt;\n  dplyr::glimpse()\n\nRows: 34,244\nColumns: 3\n$ is_water &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, TRUE, TRUE, TRUE,…\n$ sst      &lt;dbl&gt; 12.848709, 5.922903, 8.960322, 8.053871, NA, 3.865484, NA, 15…\n$ geometry &lt;POINT [°]&gt; POINT (-65.47837 40.75635), POINT (-65.71588 42.76753),…\n\n\nValues where sst are NA are beyond the scope of data present in the OISST data set, so we will take that to mean NA is land (or very nearshore). We’ll merge our bkg object and random dates (days_sample), filter to include only water.\n\nbkg = sf::st_as_sf(bkg) |&gt;\n  sf::st_set_geometry(\"geometry\") |&gt;\n  dplyr::mutate(date = days_sample, .before = 1) |&gt;\n  dplyr::filter(v$is_water)\n\nplot(sf::st_geometry(coast), extent = poly, axes = TRUE)\nplot(sf::st_geometry(poly), lwd = 2, border = 'orange', add = TRUE)\nplot(sf::st_geometry(bkg), pch = \".\", col = 'blue', add = TRUE)\n\n\n\n\nNote that the bottom of the scatter is cut off. That tells us that the sst raster has been cropped to that southern limit. We can confirm that easily.\n\nplot(sst['sst'] |&gt; dplyr::slice('time', 1), extent = poly, axes = TRUE, reset = FALSE)\nplot(sf::st_geometry(poly), lwd = 2, border = 'orange', add = TRUE)\nplot(sf::st_geometry(bkg), pch = \".\", col = \"blue\", add = TRUE)"
  },
  {
    "objectID": "covariates.html#extract-environmental-covariates-for-sst-and-wind",
    "href": "covariates.html#extract-environmental-covariates-for-sst-and-wind",
    "title": "Covariates for observations and background",
    "section": "4 Extract environmental covariates for sst and wind",
    "text": "4 Extract environmental covariates for sst and wind\n\n4.1 Wait, what about dates?\nYou may have considered already an issue connecting our background points which have daily dates with our covariates which are monthly (identified by the first of each month.) We can manage that by adding a second date, month_id, to the bkg table. We’ll use the current_month() function from the oisster package to compute that.\n\nbkg = dplyr::mutate(bkg, month_id = oisster::current_month(date))\n\n\n\n4.2 First extract background points\n\n4.2.1 Extract sst\nNow we need to read in the complete sst data. We have already read in all of the sst data, and then transform the longitude coordinates to the -180 to 180 range. Then we extract, specifying which variable in bkg is mapped to the time domain in sst - in our case the newly computed month_id matches the time dimension in sst.\n\nsst_values = stars::st_extract(sst, bkg, time_column = 'month_id')\n\n\n\n4.2.2 Extract wind\nFor wind we have three parameters (windspeed, u_wind and v_wind). Presumably windspeed is a function of u_wind and v_wind and will be correlated with them. Nonetheless, for completeness, we’ll extract all three.\n\nwindspeed_values = stars::st_extract(windspeed, bkg, time_column = \"month_id\")\n\nu_wind_values = stars::st_extract(u_wind, bkg, time_column = \"month_id\")\n\nv_wind_values = stars::st_extract(v_wind, bkg, time_column = \"month_id\")\n\n\n\n\n4.3 Now put them together and save\nNow we merge the three extractions for sst, u_wind and v_wind into one object, and then save to disk for later retrieval. It might be tempting to use merge() or one of dplyr’s join functions, but we really have an easy task as all of our extractions have the same number of records in the same order. We need only mutate bkg to include each of the extracted values.\n\nbkg = bkg |&gt;\n  dplyr::mutate(\n    sst = sst_values$sst,\n    windspeed = windspeed_values$windspeed,\n    u_wind = u_wind_values$u_wind,\n    v_wind = v_wind_values$v_wind) |&gt;\n  sf::write_sf(file.path(\"data\", \"bkg\", \"bkg-covariates.gpkg\"))\n\n\n\n4.4 Next extract observation points\nIt’s the same workflow to extract covariates for the observations as it was for the background, but let’s not forget to add in a variable to identify the month that matches those in the predictors.\n\nobs = dplyr::mutate(obs, month_id = oisster::current_month(date))\nsst_values = stars::st_extract(sst, obs, time_column = 'month_id')\nwindspeed_values = stars::st_extract(windspeed, obs, time_column = \"month_id\")\nu_wind_values = stars::st_extract(u_wind, obs, time_column = \"month_id\")\nv_wind_values = stars::st_extract(v_wind, obs, time_column = \"month_id\")\n\nobs = obs |&gt;\n  dplyr::mutate(\n    sst = sst_values$sst,\n    windspeed = windspeed_values$windspeed,\n    u_wind = u_wind_values$u_wind,\n    v_wind = v_wind_values$v_wind) |&gt;\n  sf::write_sf(file.path(\"data\", \"obs\", \"obs-covariates.gpkg\"))\n\nThat’s it! Next we can start assembling a model."
  },
  {
    "objectID": "modeling-02.html",
    "href": "modeling-02.html",
    "title": "Modeling each month",
    "section": "",
    "text": "Here we modify our first modeling workflow to produce a model for each month. In the previous workflow we produced one model covering observations covering all times; we then applied the model to the various months."
  },
  {
    "objectID": "modeling-02.html#load-data",
    "href": "modeling-02.html#load-data",
    "title": "Modeling each month",
    "section": "1 Load data",
    "text": "1 Load data\nHere we load the observation and background data points. We add a column identifying the month of the year.\n\nsource(\"setup.R\")\nobs = sf::read_sf(file.path(\"data\", \"obs\", \"obs-covariates.gpkg\")) |&gt;\n  sf::st_set_geometry(\"geometry\") |&gt;\n  dplyr::mutate(month = factor(format(month_id, \"%b\"), levels = month.abb), \n                .before = geometry)\nbkg = sf::read_sf(file.path(\"data\", \"bkg\", \"bkg-covariates.gpkg\")) |&gt;\n  sf::st_set_geometry(\"geometry\") |&gt;\n  dplyr::mutate(month = factor(format(month_id, \"%b\"), levels = month.abb), \n                .before = geometry)"
  },
  {
    "objectID": "modeling-02.html#do-we-model-every-month",
    "href": "modeling-02.html#do-we-model-every-month",
    "title": "Modeling each month",
    "section": "2 Do we model every month?",
    "text": "2 Do we model every month?\nLet’s do a quick check by counting each by month. Note that we drop the spatial info so that we can make simply tallies.\n\ncounts = sf::st_drop_geometry(obs) |&gt; \n  dplyr::count(month, name = \"n_obs\") |&gt;\n  dplyr::left_join(sf::st_drop_geometry(bkg) |&gt; dplyr::count(month, name = \"n_bkg\"), \n                   by = 'month') |&gt;\n  print(n = 12)\n\n# A tibble: 12 × 3\n   month n_obs n_bkg\n   &lt;fct&gt; &lt;int&gt; &lt;int&gt;\n 1 Jan      33  2288\n 2 Feb      40  2140\n 3 Mar      50  2312\n 4 Apr     341  2232\n 5 May     541  2305\n 6 Jun    2137  2291\n 7 Jul    2108  2371\n 8 Aug    1698  2502\n 9 Sep     725  2275\n10 Oct     328  2390\n11 Nov     494  2267\n12 Dec      66  2201\n\n\nSo the colder months have fewer observations than the warmer months. We already knew that, but it will be interesting to see how that manifests itself in the models.\n\n2.1 Build the monthly models\nSince we are building 12 models (rather than one) it is useful to create a function that computes a model for any month, and then iterate through the months of the year.\n\n# A function for making one month's model\n#\n# @param tbl a data frame of one month's observations\n# @param key a data frame that holds the current iteration's month name\n# @param bkg a complete data frame of background data (which we filter for the given month)\n# @param path the path where the model is saved\n# @return a model, which is also saved in \"data/model/v2/v2.&lt;monthname&gt;\"\nmodel_month = function(tbl, key, bkg = NULL, path = \".\"){\n  \n  bkg = bkg |&gt;\n    dplyr::filter(month == key$month) |&gt;\n    sf::st_drop_geometry() |&gt;\n    dplyr::select(dplyr::all_of(c(\"sst\", \"u_wind\", \"v_wind\"))) |&gt;\n    na.omit()\n  \n  obs = tbl |&gt;\n    sf::st_drop_geometry() |&gt;\n    dplyr::select(dplyr::all_of(c(\"sst\", \"u_wind\", \"v_wind\"))) |&gt;\n    na.omit()\n  \n  # these are the predictor variables row bound\n  x = dplyr::bind_rows(obs, bkg)\n  \n  # and the flag indicating presence/background\n  flag = c(rep(1, nrow(obs)), rep(0, nrow(bkg)))\n  \n  model_path = file.path(path, paste0(\"v2.\", key$month, \".rds\"))\n\n  model = maxnet::maxnet(flag, x) |&gt;\n    maxnetic::write_maxnet(model_path)\n                         \n  model\n}\n\npath = file.path(\"data\", \"model\", \"v2\")\nok = dir.create(path, recursive = TRUE, showWarnings = FALSE)\nmodels = obs |&gt;\n  dplyr::group_by(month) |&gt;\n  dplyr::group_map(model_month, bkg = bkg, path = path) |&gt;\n  rlang::set_names(levels(obs$month))\n\nWe can look at the response plots for every month, but for demonstration purposes, we’ll just show one month. It is interesting to compare this respinse to that for the basic model.\n\nplot(models[['Jun']], type = 'cloglog')"
  },
  {
    "objectID": "modeling-02.html#predict-with-rasters",
    "href": "modeling-02.html#predict-with-rasters",
    "title": "Modeling each month",
    "section": "3 Predict with rasters",
    "text": "3 Predict with rasters\nFirst we load the raster databases as these are lightweight to pass into a function that iterates through the months.\n\n3.1 Load the raster databases (sst and u_wind and v_wind)\nWe also make sure they are in date order and add a “month” variable to each.\n\nsst_path = \"data/oisst\"\nsst_db = oisster::read_database(sst_path) |&gt;\n  dplyr::arrange(date) |&gt;\n  dplyr::mutate(month = format(date, \"%b\"))\n  \n\nnbs_path = \"data/nbs2\"\nwind_db = nbs::read_database(nbs_path) |&gt;\n  dplyr::arrange(date)|&gt;\n  dplyr::mutate(month = format(date, \"%b\"))\n\nu_wind_db = wind_db |&gt;\n  dplyr::filter(param == \"u_wind\")\n\nv_wind_db = wind_db |&gt;\n  dplyr::filter(param == \"v_wind\")\n\n\n\n3.2 Iterate through the months making predictions\nNow we can build an iterator function that will make a prediction for each month. Let’s narrow our predictions to just those for a particular year, 2019, and read the rasters in all at once.\n\ndates = as.Date(c(\"2019-01-01\", \"2019-12-31\"))\nx = read_predictors(\n  sst_db = dplyr::filter(sst_db, dplyr::between(date, dates[1], dates[2])),\n  u_wind_db = dplyr::filter(u_wind_db, dplyr::between(date, dates[1], dates[2])),\n  v_wind_db = dplyr::filter(v_wind_db, dplyr::between(date, dates[1], dates[2])) )\n\nNow we can iterate through the months.\n\ndate_sequence = seq(from = dates[1], to = dates[2], by = \"month\")\npred_rasters = lapply(names(models),\n  function(mon){\n    ix = which(month.abb %in% mon)\n    predict(models[[mon]], dplyr::slice(x, time, ix, drop), type = \"cloglog\")\n  }) \npred_rasters = do.call(c, append(pred_rasters, list(along = list(time = date_sequence))))\n\nLet’s plot them.\n\ncoast = rnaturalearth::ne_coastline(scale = 'large', returnclass = 'sf') |&gt;\n  sf::st_geometry() |&gt;\n  sf::st_crop(pred_rasters)\n\nplot_coast = function() {\n  plot(coast, col = 'green', add = TRUE)\n}\nplot(pred_rasters, hook = plot_coast)\n\n\n\n\nLet’s see what we can discern from the predict abilities. We can extract the predicted values at the observed locations. Having those in hand allows us to compute pAUC for each month.\n\npred_obs = stars::st_extract(pred_rasters, \n                             dplyr::filter(obs, dplyr::between(date, dates[1], dates[2])),\n                             time_column = \"month_id\") |&gt;\n  dplyr::mutate(month = factor(format(month_id, \"%b\"), levels = month.abb)) |&gt;\n  dplyr::group_by(month)\n\npaucs = dplyr::group_map(pred_obs,\n                        function(x, y) {\n                          ix = month.abb %in% y$month\n                          s = dplyr::slice(pred_rasters, \"time\", ix)\n                          pauc = maxnetic::pAUC(s,x)\n                          dplyr::tibble(month = y$month, \n                                        auc = pauc$area,\n                                        pauc = list(pauc))\n                        })|&gt;\n  dplyr::bind_rows() |&gt;\n  print(n = 12)\n\n# A tibble: 12 × 3\n   month   auc pauc      \n   &lt;fct&gt; &lt;dbl&gt; &lt;list&gt;    \n 1 Jan   0.684 &lt;pAUC [3]&gt;\n 2 Feb   0.735 &lt;pAUC [3]&gt;\n 3 Mar   0.693 &lt;pAUC [3]&gt;\n 4 Apr   0.719 &lt;pAUC [3]&gt;\n 5 May   0.704 &lt;pAUC [3]&gt;\n 6 Jun   0.706 &lt;pAUC [3]&gt;\n 7 Jul   0.697 &lt;pAUC [3]&gt;\n 8 Aug   0.730 &lt;pAUC [3]&gt;\n 9 Sep   0.696 &lt;pAUC [3]&gt;\n10 Oct   0.676 &lt;pAUC [3]&gt;\n11 Nov   0.661 &lt;pAUC [3]&gt;\n12 Dec   0.659 &lt;pAUC [3]&gt;\n\n\nNote that last element, pauc, is the result returned by the maxnetic::pAUC() function which we can plot.\n\npp = paucs |&gt;\n  dplyr::group_by(month) |&gt;\n  dplyr::group_map(\n    function(tbl, key){\n      plot(tbl$pauc[[1]], title = key$month, xlab = \"\", ylab = \"\")\n    }\n  )\npatchwork::wrap_plots(pp, ncol = 4)\n\n\n\n\nWell, it would be easy to become dispirited by this result. It would be reasonable to expect AUC values to improve if we built monthly models rather than a single model applied to any month. But it seems to not be the dramatic improvement hoped for. Darn!"
  },
  {
    "objectID": "modeling-04.html",
    "href": "modeling-04.html",
    "title": "Background count",
    "section": "",
    "text": "We arbitrarily chose to select 4 background points for every observation. But what is the best number to pick? There is no rule about this, and what number is best may be context dependent.\nHere we show some simple steps to determine a best number of background points for a given workflow. We’ll build a series of models selecting a different number of background points each time, and then we’ll compare the results.\nFirst, we’ll load the observations and background data.\n\nsource(\"setup.R\")\n\nobs = sf::read_sf(file.path(\"data\", \"obs\", \"obs-covariates.gpkg\")) |&gt;\n  na.omit()\nbkg = sf::read_sf(file.path(\"data\", \"bkg\", \"bkg-covariates.gpkg\")) |&gt; \n  na.omit()\n\nThis time we want to make a a single model that covers May through October. We need to filter out the records by month, so first we’ll make a month_name variable.\n\nmonths = month.abb[5:10]\nobs = obs |&gt;\n  dplyr::mutate(month_name = factor(format(month_id, \"%b\"),\n                                    levels = month.abb)) |&gt;\n  dplyr::filter(month_name %in% months)\n\nbkg = bkg |&gt;\n  dplyr::mutate(month_name = factor(format(month_id, \"%b\"),\n                                    levels = month.abb)) |&gt;\n  dplyr::filter(month_name %in% months)\n\nLet’s make a ‘standard summer’ to use for prediction by computing the mean sst, u_wind and v_wind.\n\nsst_path = \"data/oisst\"\nsst_db = oisster::read_database(sst_path) |&gt;\n  dplyr::arrange(date) |&gt;\n  dplyr::mutate(month = format(date, \"%b\")) |&gt;\n  dplyr::filter(month %in% months)\n  \n\nwind_path = \"data/nbs\"\nwind_db = nbs::read_database(wind_path) |&gt;\n  dplyr::arrange(date)|&gt;\n  dplyr::mutate(month = format(date, \"%b\"))|&gt;\n  dplyr::filter(month %in% months)\n\nu_wind_db = wind_db |&gt;\n  dplyr::filter(param == \"u_wind\")\n\nv_wind_db = wind_db |&gt;\n  dplyr::filter(param == \"v_wind\")\n\nThis is getting into the weeds a bit, but it is typical to encounter issues with inout data that simply must be resolved. In this case, it turns out that NBS wind data is missing a number of months which are not missing in the OISST data.\n\ncat(sprintf(\"sst: %i rows, u_wind: %i rows, v_wind: %i rows\", nrow(sst_db),\n        nrow(u_wind_db), nrow(v_wind_db)), \"\\n\")\n\nsst: 143 rows, u_wind: 142 rows, v_wind: 142 rows \n\n\nSo we have to filter out of sst and u_wind any dates not found in v_wind data.\n\nsst_db = dplyr::filter(sst_db, date %in% v_wind_db$date)\nu_wind_db = dplyr::filter(u_wind_db, date %in% v_wind_db$date)\ncat(sprintf(\"sst: %i rows, u_wind: %i rows, v_wind: %i rows\", nrow(sst_db),\n        nrow(u_wind_db), nrow(v_wind_db)), \"\\n\")\n\nsst: 142 rows, u_wind: 142 rows, v_wind: 142 rows \n\n\nOK! Now we can read in the predicitors and compute a mean* (for the data we have in hand which is a shortcoming).\n\npredictors = read_predictors(sst_db = sst_db,\n                              u_wind_db = u_wind_db,\n                              v_wind_db = v_wind_db) |&gt;\n  stars::st_apply(c(\"x\", \"y\"), mean, na.rm = TRUE) |&gt;\n  print()\n\nstars object with 2 dimensions and 3 attributes\nattribute(s):\n              Min.    1st Qu.     Median       Mean   3rd Qu.      Max. NA's\nsst     10.5602205 16.7377749 22.7769238 21.0014044 24.972773 27.554115  820\nu_wind  -1.0042610  1.1206482  1.6890050  1.5675090  2.037007  2.662161  573\nv_wind  -0.2938452  0.4550395  0.8481969  0.9446332  1.542540  2.151062  573\ndimension(s):\n  from to offset delta refsys point x/y\nx    1 74 -76.38  0.25 WGS 84 FALSE [x]\ny    1 46  46.12 -0.25 WGS 84 FALSE [y]\n\n\nNext we can make a ‘standard’ summer by computing the mean for each attribute.\n\npredicitors = stars::st_apply(predictors, c(\"x\", \"y\"), mean, \n                              na.rm = TRUE) |&gt;\n  print()\n\nstars object with 2 dimensions and 3 attributes\nattribute(s):\n              Min.    1st Qu.     Median       Mean   3rd Qu.      Max. NA's\nsst     10.5602205 16.7377749 22.7769238 21.0014044 24.972773 27.554115  820\nu_wind  -1.0042610  1.1206482  1.6890050  1.5675090  2.037007  2.662161  573\nv_wind  -0.2938452  0.4550395  0.8481969  0.9446332  1.542540  2.151062  573\ndimension(s):\n  from to offset delta refsys point x/y\nx    1 74 -76.38  0.25 WGS 84 FALSE [x]\ny    1 46  46.12 -0.25 WGS 84 FALSE [y]\n\n\nNow we can make the models for those months, but we need to establish the number of background points for each model. Let’s do a simple progression… from 10 to 10000. We’ll build a model for each number of background points, make a predictive map and then compute AUC.\nComments in the code block below help explain the steps taken.\n\n# choose a sequence of background counts\nnback = c(10, 20, 50, 100, 250, seq(from = 500, to = 10000, by = 500))\n# iterate through each count\nx = lapply(nback,\n  # for each count apply this function where\n  # @param nbk count of background to use\n  # @param ob the full observation set\n  # @param bk the full background set\n  # @param preds the predictor rasters\n  # @return a one row tibble with count and presence AUC\n  function(nbk, ob = NULL, bk = NULL, preds = NULL){\n    # prepare all of the observations\n    obn = dplyr::select(ob, dplyr::all_of(c(\"sst\", \"u_wind\", \"v_wind\"))) |&gt;\n      sf::st_drop_geometry()\n    # prepare and then sample the background\n    bkn = dplyr::select(bk, dplyr::all_of(c(\"sst\", \"u_wind\", \"v_wind\"))) |&gt;\n      sf::st_drop_geometry() |&gt; \n      dplyr::slice_sample(n=nbk)\n    # make the vector that identifies obs-vs-bkg \n    flag = c(rep(1, nrow(obn)), rep(0, nrow(bkn)))\n    # create the input data in the same order as the flag\n    input = dplyr::bind_rows(obn, bkn)\n    # build model!\n    model = maxnet::maxnet(flag, input)\n    # predict!\n    p = predict(model, preds, type = 'cloglog')\n    # compute pAUC\n    pauc = maxnetic::pAUC(p, ob, time_column = NULL)\n    # return a tibble\n    dplyr::tibble(nbkg = nbk, AUC = pauc$area)\n  }, ob = obs, bk = bkg, preds = predictors) |&gt;\n  # bind all of the individual tibbles into one data frame (still a tibble)\n  dplyr::bind_rows() |&gt;\n  dplyr::glimpse()\n\nRows: 25\nColumns: 2\n$ nbkg &lt;dbl&gt; 10, 20, 50, 100, 250, 500, 1000, 1500, 2000, 2500, 3000, 3500, 40…\n$ AUC  &lt;dbl&gt; 0.8173758, 0.8174116, 0.8231246, 0.8306761, 0.8190706, 0.8196828,…\n\n\nNow we can show these with a simple plot.\n\n\nCode\nggplot2::ggplot(data = x, ggplot2::aes(x = nbkg, y = AUC)) +\n  ggplot2::geom_point() + \n  ggplot2::geom_smooth(method = \"loess\", se = TRUE, formula = y ~ x) + \n  ggplot2::labs(x = \"Number of Background Points\",\n                title = \"Choosing the number of background points\")\n\n\n\n\n\nThis workflow reveals to us that for the seasonal May-Oct model that we get no improvement in AUC once we get to 2500 background points.\n\n\n\n Back to top"
  }
]